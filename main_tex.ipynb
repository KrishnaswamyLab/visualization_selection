{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scottgigante/.local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import scprep\n",
    "from blog_tools import data, embed, interact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "#### What is good visualization?\n",
    "\n",
    "Despite humans only being capable of interpreting data in three dimensions, real-world data often exists in much higher dimensions, making the task of understanding the structure of such data difficult. As a result, exploring high-dimensional data typically involves some form of dimensionality reduction to two or three dimenisons for visualization. The goal of this technique is to gain insight about the structure of the data, specifically the relationships between points. From these observations, one can generate hypotheses about the dataset. In this way, visualization is an important tool for narrowing the scope of investigation and aids in the selection of tools for future analysis. The importance of this problem is reflected in the many visualization tools have been developed for high-dimensional data. To name a few, we have: [Principal Components Analysis](https://www.sciencedirect.com/science/article/pii/0169743987800849), [Multidimensional Scaling](https://link.springer.com/article/10.1007/BF02289565), [Diffusion Maps](https://www.sciencedirect.com/science/article/pii/S1063520306000546), [Isometric Mapping](https://science.sciencemag.org/content/290/5500/2319.abstract), [Laplcaian Eigenmaps](https://www.semanticscholar.org/paper/Laplacian-Eigenmaps-for-Dimensionality-Reduction-Belkin-Niyogi/0ce8879ea7fc0e96fd0e4e242a46002010f86e18), [Locally Linear Embedding](https://science.sciencemag.org/content/290/5500/2323.full), [t-distributed Stochastic Neighbor Embedding](http://www.jmlr.org/papers/v9/vandermaaten08a.html), [Uniform Manifold Approximation](https://arxiv.org/abs/1802.03426), and [Potential of Heat Affinity Transition Embedding](https://www.biorxiv.org/content/10.1101/120378v4) to name a few. However, along with a diversity of tools comes a diversity of associated biases and assumptions that these methods introduce, and the dreaded problem of selecting the right tool for the job.\n",
    "\n",
    "Take, for example, t-SNE. At the time of writing, [Visualizing Data using t-SNE (2008)](http://www.jmlr.org/papers/v9/vandermaaten08a.html) has over 8,400 citations. In some fields, like computational biology, it's difficult to find a published paper without a t-SNE visualization. However, t-SNE has a unique set of biases that lend it to misinterpretation; these limitations are explored in the Distill article [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/). Through a set of key examples, the authors show how parameter selection and intrinsic biases in the t-SNE algorithm can lead to misleading visualizations. This article has become a widely referenced teaching tool for the rational application of the method. \n",
    "\n",
    "Here, we seek to take the question of *How to Use t-SNE Effectively* and go one step further: How can you select among the many visualization tools effectively? How can you judge the results of each method? What benchmarks should one employ when considering different methods? Which methods are best suited for different data modalities? The remainder of this article will be divided into three sections. \n",
    "\n",
    "#### Structure of the article\n",
    "\n",
    "1. [Selecting toy datasets for comparing methods](#selecting-toy-datasets-for-comparing-methods)\n",
    "\n",
    "First, we will talk about different kinds of data structures and introduce a few toy datasets that we will use to compare different visualization methods. Next, we will use these datasets to discuss the algorithms behind six popular dimenisonality reduction methods: PCA, MDS, ISOMAP, t-SNE, UMAP, and PHATE.\n",
    "\n",
    "2. [Parameter selection](#parameter-selection)\n",
    "\n",
    "We will then discuss sensitivity to parameter choices and methods for quanitfying the accuracy of a visualization method. \n",
    "\n",
    "**TODO: quantification?**\n",
    "\n",
    "3. [Real world data](#real-world-data)\n",
    "\n",
    "Finally, we will apply our seven comparison methods to large, real-world datasets. The emphasis of this section will be how a particular visualiation tool biases the observer towards a set of conclusions about a given dataset.\n",
    "\n",
    "#### Dimensionality reduction vs. visualization\n",
    "\n",
    "Before we go further, it will be useful to clarify the distinction between dimensionality reduction algorithms and visualization algorithms. In fact, visualization algorithms are a subset of dimensionality reduction techniques. The important distiction here is that some dimensionality reduction algorithms are useful for reducing computational complexity of some machine learning tasks, but are not good for creating interpretable two- or three-dimensional visualizations. Take for example, dimensionality reduction by random orthogonal projection, often used in [approximate nearest-neighbors search](https://dl.acm.org/citation.cfm?id=276877). This method take a set of $n$ random orthogonal vectors in the original $d$-dimensional data space with $n << d$. This approach preserves most of the structure of the data so that statistics (such as pairwise distances) can be calculated more quickly. However, random projections evenly preserves information across all $n$ dimensions and will not product features useful for visualizing the relationships between data points. To count as a visualization tool, an algorithm must prioritize information preservation in 2 or 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting toy datasets for comparing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rigorous application of any computational technique starts with considering the desired application and how well a given method's biases fits that application. Generally, a good place to start is by creating *toy data*. A good toy data set is small, easy to understand intuitively, and has a clear heuristic for a sucessful visualzation. In *How to Use t-SNE Effectively*, the authors present several compelling toy datasets. Here, we will consider a few of these along with some extras: a graph, small collection of handwritten 7's, and [2000 pictures of Brendan Frey's face](https://cs.nyu.edu/~roweis/data/frey_rawface.jpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ground truth images of six datasets](img/ground_truth.png)\n",
    "\n",
    "**TODO: The tree image actually has another branch now. See the PHATE image below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected these datasets because of the varying structures and modalites. \n",
    "\n",
    "* **Swiss roll:** This dataset is adapted from `sklearn`'s `dataset.make_swiss_roll()` method. The data exists in three dimensions with one latent dimension: the distance of each point from the center. This is represented by the color of the points in the above plots. The first two dimensions are generated using sines and cosines of this latent dimension and the third dimension is generated as uniform random noise. In this dataset, the noise dimension is slightly larger than the two shown above. As we will see, this proves problematic for some algorithms. The \"ideal\" visualization of the swiss roll is a single line that represents the latent dimension of that dataset.\n",
    "\n",
    "  \n",
    "* **Three blobs:** The three blobs here are gaussian clouds that were first generated in just two dimensions and then linearly projected into 200 dimensions using random Gaussian noise. In the original space, the ratio between the orange and blue blobs is 1/5th the distance between orange and green blobs. The goal here is to preserve the relative distances between the blobs while rotating the data back into two dimensions.\n",
    "\n",
    "  \n",
    "* **Uneven circle:** Here, the data points are distributed along a circle in 20 dimensions, but the spacing between the points is irregular. The color of each point represents it's angular position.\n",
    "\n",
    "\n",
    "* **Digits:** This is the dataset found in `sklearn.datasets.load_digits()` method. According to the User Guide, the data is a copy of the test set of [the UCI ML hand-written digits dataset](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits). We're just using the 179 sevens. Each image is 8x8 pixels. Because the digits are handwritten, there are natural variations in the digits. For example, some images have a cross-bar and other do not.\n",
    "\n",
    "  \n",
    "* **Frey faces:** This dataset consists of 200 images of Brendan Frey's face. In this dataset, Frey is making a series of facial expressions and many intermediate expressions are available between images. Here, we should be able to identify some continuous progressions in facial expression. For example, we should be able to see a smooth transition between a neutral face and a smile or frown.\n",
    "\n",
    "  \n",
    "* **Tree:** For our final dataset, we look at a collection of points arranges in a tree, generated using the tool [**Splatter**](https://bioconductor.org/packages/release/bioc/html/splatter.html). This data emulates smooth transitions between various states of biological systems. All of the branches of the tree are of equal length, but the points are not spaced evenly along the branches. Furthermore, some of the features change non-linearly along branches. The goal here is to recreate the topology of the six branches shown above.\n",
    "\n",
    "  \n",
    "\n",
    "With these toy datasets in hand, we can begin to compare methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing... _the algorithms_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this blog, we wanted to focus on a mix of classic and popular algorithms. Not many people use PCA for rigorous visualization of high-dimensional data (looking at you, [population geneticists](https://blog.insito.me/why-pca-and-genetics-are-a-match-made-in-heaven-6042ea027cf0)), but it is a common tool for preliminary analysis. Similarly, MDS, which is actually a collection of methods, is not as frequently applied for data analysis but serves as a foundation for non-linear dimensionality reduction. On the other hand, PHATE is a relatively new method for visualizing high dimensional data that chose to include because we developed it.\n",
    "\n",
    "In this section, we will break introduce the algorithms one by one, give a brief overview of how the algorithm works, then show the results of that algorithm on our test cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = embed.__all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e55a3157dc4c48af6611c2b2c9f522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': 'PCA', '1': 'MDS', '2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_widget = interact.TabWidget()\n",
    "for algorithm in algorithms:\n",
    "    name = algorithm.__name__\n",
    "    with tab_widget.new_tab(algorithm.__name__):\n",
    "        interact.display_markdown(\"md/discussion-{}.md\".format(name),\n",
    "                                  placeholder='Introduction to {}'.format(name))\n",
    "\n",
    "tab_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis\n",
    "\n",
    "#### How does PCA work?\n",
    "\n",
    "PCA and related eigendecomposition methods are some of the most fundamental dimensionality reduction tools in data science. Many methods, including tSNE, UMAP, and PHATE, first reduce the data using PCA before performing further operations on the data.\n",
    "\n",
    "You can find many rigorous descriptions of the PCA algorithm online. Here, we will focus on the intutition. The goal of PCA is to identify a set of orthogonal dimensions (each of which is a linear combination of the input features) that explain the maximum variance in the data. Generally, this is performed by taking the eigenvectors of the covariance matrix of the data and ordering them by decreasing eigenvalues. If you're not so familiar with eigendecomposition, it's important to remember that this method identifies a set of orthogonal linear combinations of the columns of a matrix. If you consider the interpretation of eigenvalues as defining the amount of scaling of an eigenvector when multiplied by a matrix, then it makes sense that the eigenvector that is scaled the most when multiplied by the covariance matrix represents an axis of maximal variance within the data.\n",
    "\n",
    "One of the major drawbacks of PCA is that dimensions used for visualization can only represent linear combinations of the feature space. This means that if there is non-linearity in the data, PCA will struggle to accruately represent this data structure in two or three dimensions.\n",
    "\n",
    "#### PCA on toy data cases\n",
    "\n",
    "![PCA - toy data](img/toy_data.PCA.png)\n",
    "\n",
    "First, let's consider the **swiss roll**. Notice how the latent dimension -- the distance from the center of the spiral -- is not well represented by these first two dimensions? This is because the noise dimension is wider than the swiss roll is tall. This brings us to our first important insight about PCA. Because PCA finds linear combinations of the data features with maximal variance, if the dimension of largest variance is noise, then PCA will show the noise.\n",
    "\n",
    "Furthermore, remember that our goal with the swiss roll was to \"unroll\" the data and show the data lying on a single line or a 2d plane. Because the data is non-linear, there's no way for PCA to perform this unfurling. We'll get back to this later when we look at datasets that can unroll the roll, but for now, remember that PCA can hide non-linear relationships between the data.\n",
    "\n",
    "Second, we look at the **three blobs** dataset. Here, PCA did a good job of capturing the positioning of the data. This should be expected because the input data is a linear projection of the latent data. PCA is essentially just a rotation, and the latent space of the data here is 2-dimensional so PCA can well represent the data.\n",
    "\n",
    "In the case of the **uneven circle**, we see that the data structure is more or less reasonably represented due to the same reasons as in the three blobs case. However, notice that the uneven spacing of the data is lost.\n",
    "\n",
    "For the **digits**....\n",
    "\n",
    "With the **frey faces**....\n",
    "\n",
    "Examining the **tree** dataset, we can see that PCA failed to show the six branches in two dimensions and depicts many branches overlapping. If we were to be looking only at the data without the color added, you might guess that there are only two branches in this data. This squashing of branches is because the underlying data structure does not exist as two linear combinations of the data features. Different features change between branches, so one would need one pricinpal component per branch to represent this data. Furthermore, the non-linearity of the changes in the features of each branch causes PCA to make some branches appear curves. Although this is true with respect to the feature space, this is unfaithful to the latent data generative space. As with the Swiss Roll, PCA has no way of \"unfurling\" the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS (Multidimensional scaling)\n",
    "\n",
    "#### How does MDS work?\n",
    "\n",
    "Unlike the other methods here, MDS actually refers to a collection algorithms. The algorithms are often broken down into three categories: classsical, metric, and non-metric. You can find full descriptions and comparison of these algorithms [elsewhere](https://www.springer.com/gp/book/9780387251509), but here we will be using a metric MDS algorithm implemented in `sklearn.manifold.MDS`. A quick intutitive explanation of metric MDS is that minimizies the difference between high-dimensional euclidean distances and low-dimensional euclidean distances. The specific algorithm used in `sklearn` is called [SMACOF](https://escholarship.org/uc/item/4ps3b5mj), which minimizes the following cost function:\n",
    "\n",
    "$$ \\sigma (X) = \\sum_{i,j \\in X} [d(X_{i,j}) - d(x_{i,j})]^2 $$\n",
    "\n",
    "Here, $X$ is the data in high dimensional space and $x$ is the distance in low dimensional space and $d$ is the Euclidean distance function. To solve this problem, `sklearn` starts with a random configuration of low-dimensional embeddings, assigning each cell a random coordinate in the low-dimensional space. Next, all pairwise distances in the low-dimensional space are calculated and the ratio between the high- and low-dimensional spaces are calculated. Finally, all points are adjusted in the low-dimensional space such that the discrepancy in distances is minimized. This process is repeated until convergence. One of the nice parts about SMACOF is that it is guarenteed to monotonically decrease stress. However, there is no guarentee that the final solution is not a local minimum. To address this, `sklearn` runs MDS several times and takes the best solution.\n",
    "\n",
    "#### MDS on toy datasets\n",
    "\n",
    "![MDS - toy data](img/toy_data.MDS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swiss roll** - MDS performs similarly to PCA in embedding the Swiss roll because it preserves all pairwise distances. Although MDS is non-linear, the cost function is linear across distance scales, so it will never \"unroll\" the swiss roll. That behavior would require the ability to understand that it doesn't matter how close the center and outside of the swiss roll are placed in the low-dimensional space.\n",
    "\n",
    "**Three blobs** - Again, for similar reasons that PCA performs well here, MDS does a good job of embedding the three clusters. The ratio of blue:orange and blue:green blobs is preserved, and the blobs look like blobs.\n",
    "\n",
    "**Uneven circle** - This representation is fairly accurate, although the uneven spacing is lost.\n",
    "\n",
    "**Tree** - MDS does very poorly on the tree, perhaps becuase the data is very high dimensional (500 dimensions) and noisy. This is likely an initialization issue: the probability that the global minimum is reached from a set of random initializations is low. A possible solution here would be to initalize MDS using PCA or some other embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISOMAP (ISOmetric MAPping)\n",
    "\n",
    "#### How does ISOMAP work?\n",
    "\n",
    "ISOMAP is conceptually similar to the classical MDS algorithm which takes an eigendecomposition of a double-centered squared Euclidean distance matrix. A drawback of the classical MDS approach is that the method is that global distances are calculated linearly in the ambient space. As we've seen above, when the data lies on a non-linear manifold, such as a swiss roll, large Euclidean distances do not correspond to manifold distances. To solve this problem, ISOMAP considers *geodesic distances* as a measure of dissimilarity between points. Geodesic distances quantify distances walking along a graph learned on the data. This is akin to taking distances along the data manifold. On the swiss roll, this means that the points farthest from eachother will be the points in the center and outside of the roll.\n",
    " \n",
    "A simple approach to building a graph given is a set of data points is to connect a each point to it's *k* Nearest Neighbors. Geodesic distances along the resulting kNN graph can be calculated as the shortest path between a given pair of points. Next, the geodesic distance matrix is double-centered and eigendecomposed with the first 2 or 3 eigenvectors used for visualization. Conveniently, `sklearn` takes care of all of the graph building and shortest path calculations so that ISOMAP dimensions can be easily calculated from any dataset.\n",
    "\n",
    "#### ISOMAP on toy data\n",
    "\n",
    "![ISOMAP - toy data](img/toy_data.ISOMAP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swiss roll** - As expected, ISOMAP works very well on the swiss roll. Becuase distances are taken along the data manifold, the swiss roll is effectively unrolled and we can easily identify the two meaningful axes through the data.\n",
    "\n",
    "**Three blobs** - One of the issues with ISOMAP can be easily observed when the data exists as more than one manifold. The issue here is that the distances between the clusters is essentially undefined. The default graph learned by ISOMAP here obviously failed to connect the three blobs, so the algorithm doesn't preserve the undefined distances between them. Each of the first three eigenvectors of the dissimilarity matrix will represent the lowest frequency axis through each individual cluster. These low-frequency eigenvectors are the lines that thepoints from each blob have been projected on. If we were to look at three dimensions, we would see that the orange blob would be represented as a line orthogonal to the blue and green.\n",
    "\n",
    "**Uneven circle** - This representation is fairly accurate, although the uneven spacing is lost because the kNN graph adjusts to varying data density along the circle.\n",
    "\n",
    "**Tree** - One issue with ISOMAP is that it relies on accurate geodesic distances between points. Here, the kNN graph appears to have failed to capture the true geometry of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE (t-distributed Stochastic Neighbor Embeding)\n",
    "\n",
    "#### How does t-SNE work?\n",
    "\n",
    "One can find many articles with a formal treatment of the t-SNE algorithm. The intuition behind the method is that t-SNE finds a transformation of the data from the feature space to 2 or 3 dimensions such that the distances between points within any given *local neighborhood* are preseved. In t-SNE, a local neighborhood is defined using a kernel funciton defined by the Student's t distribution and distance preservation is quantified using KL-divergence between the distance matrix in high dimensions and low dimensions. The exact embedding is calculated using stochastic gradient descent. To learn more, consult the [original paper](www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), or any number of [online explanations](http://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/).\n",
    "\n",
    "There are a couple of key points to consider with t-SNE. First, the embedding is calculated using only local neighborhood distances. This means that t-SNE will not preserve any information that cannot be learned using overlapping neighborhoods. As a result, the reduced dimensions learned by t-SNE *do not represent a metric space.* This means that pairwise distances computed in t-SNE space do not respect, among other things, the triangle inequality. Data points with equal values in the feture space [may not be positioned in the same point in the t-SNE space](https://datascience.stackexchange.com/questions/19025/t-sne-why-equal-data-values-are-visually-not-close). Furthermore, the distances between clusters that one sees in a tSNE embedding are meaningless (https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne). For a full treatment of the limitations of t-SNE, consult [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "#### t-SNE on toy data cases\n",
    "\n",
    "![t-SNE - toy data](img/toy_data.TSNE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swiss roll** - Here, t-SNE does terribly. This poor performance has been noted before ([see this comparison of SNE methods on the swiss roll](https://jlmelville.github.io/smallvis/swisssne.html)), and is even addressed on [the t-SNE FAQ](https://lvdmaaten.github.io/tsne/#faq). Laurens van der Maaten's rationale for this is that the swiss roll has low instrinsic dimensionality and so does not suffer from the so-called \"crowding problem\" discussed in the [original paper](www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).  He also writes, \"who cares about Swiss rolls when you can embed complex real-world data nicely?\" However, we note that it is difficult to define \"nicely\" for real-world data.\n",
    "\n",
    "**Three blobs** - t-SNE performs okay on this dataset. The blobs look like blobs, and the green blob is slightly closer to the blue than orange blob. However, the ratio between green/blue vs green/orange blobs is nowhere near 1:5.\n",
    "\n",
    "**Uneven circle** - Here, the result from t-SNE is partly faithful to the original data, but the noise added to the original data is being represented as width whereas the data was originally generated one-dimensionally along the circle. Also, the uneven spacing of the points on the circle is lost in this embedding.\n",
    "\n",
    "**Tree** - Because t-SNE only preserves local neighborhood distances, it has a tendency to shatter data distributed as a tree. This occurs because there are proportionally few cells at the points where two or more branches meet. As such, the penalty for failing to preserve the distances at these regions in the data is small. Additionally, t-SNE performs poorly with outliers, which are added by Splatter at a 5% frequency by default. Here you can see that a few of these outliers are place very far from the main data, skewing the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP (Uniform Manifold Approximation)\n",
    "\n",
    "#### How does UMAP work?\n",
    "\n",
    "UMAP is a recently developed dimensionality reduction algorithm that has seen a recent explosion of popularity. UMAP shares some conceptual similarites to both tSNE and force-directed layout (a popular method for visualizing graphs). The algorithm is described in [an ArXiv pre-print](https://arxiv.org/abs/1802.03426). UMAP has an excellent set of tutorials and usage examples on github, along with stellar documentation of the code. Unfortunately, we (and [others](https://www.math.upenn.edu/~jhansen/2018/05/04/UMAP/)) find that the description of the UMAP algorithm is difficult to parse. The authors use jargon like \"fuzzy topological representations\" and \"1-simplices\" where the canonical terms \"graph\" and \"edge\" would suffice. However, the basics of the algorithm are fairly straightforward.\n",
    "\n",
    "UMAP starts by building a graph from the data using an adjustable bandwidth kernel set using approximate nearest-neighbors. Next, UMAP uses gradient descent to minimize the cross-entropy between the edge weights in high dimensions and the edge weights constructed on a graph in low dimensions. That's it!\n",
    "\n",
    "Let's see how UMAP performs.\n",
    "\n",
    "#### UMAP on toy data cases\n",
    "\n",
    "![UMAP - toy data](img/toy_data.UMAP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swiss roll** - Here we observe that UMAP does a fair job of unrolling the swiss roll. Two issues with this embedding are that one part of the roll that is broken apart and another that is split off from the main group of points. Overall, this looks far better than t-SNE or PCA.\n",
    "\n",
    "**Three blobs** - Because UMAP shares such conceptual similarity with t-SNE, it's not surprised that it performs similarly on these blobs. The exact ratio spacing between the blue:orange and blue:green clusters is lost, and the orangle blob, which should be on the outside of the plot, is instead place in between them. \n",
    "\n",
    "**Uneven circle** - Here, UMAP fails to embed the circle properly and breaks it into a single lines additionally, the uneven spacing between points is lost.\n",
    "\n",
    "\n",
    "**Tree** - Although UMAP aims to perserve local and global distances, you can see that the algorithm suffers from the same difficulties as t-SNE. Because the algorithm tries to preserve local distances over global distances, the algorithm has a tendency to shatter a tree at the branching points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE (Potential of Heat Affinity Transition Embedding)\n",
    "\n",
    "#### How does PHATE work?\n",
    "\n",
    "PHATE is a method designed by us to solve many of the issues that we described in the above visualization methods. PHATE is inspired by [diffusion maps](https://www.sciencedirect.com/science/article/pii/S1063520306000546) and was developed in collaboration with Dr. Coifman. To understand PHATE, it is useful to first describe diffusion maps. Diffusion maps aim to preserve *diffusion distances* between points. These diffusion distances approximate the rate of heat flow along a graph, and can be aproximated by random walks. Dense regions of the graph have smaller diffusion distances between points than sparse regions. In diffusion maps, the Markov-normalized weight matrix $M$, called the diffusion operator, is powered to some value $t$ and then the first 2 or 3 eigenvectors of $M^t$ are used for visualization.\n",
    "\n",
    "Although the diffusion map approach is very successful at capturing non-linear data geometries, it often fails to preserve global structure in only 2 or 3 dimensions and requires more dimensions as the dataset becomes more complex. If a dataset is distributed on a tree, diffusion maps often require one dimension per branch to preserve global distances. As such, diffusion maps is an excellent dimensionality reduction algorithm, but it is not so useful for visualization.\n",
    "\n",
    "To solve this problem of capturing global structure in 2 or 3 dimensions, PHATE aims to preserve *potential distances* between points. The diffusion distances used in diffusion maps consider the euclidean distance between rows of $M^t$. This means that the large fold-change differences in long range distances (*i.e.* transition probabilities of 0.01 vs 0.03) are basically ignored in the face of similar fold-change differences in among local neighborhoods (*i.e.* transition probabilities of 0.1 vs 0.3). To ensure that PHATE is sensitive to such global structure, PHATE first applies a negative log transform prior to calculating euclidean distances. These potential distances are then embedded in 2 or 3 dimensions using the distance-preserving MDS algorithm described above.\n",
    "\n",
    "#### PHATE on toy data\n",
    "\n",
    "![toy data - PHATE](img/toy_data.PHATE.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swiss roll** - The PHATE visualization of the swiss roll looks almost as good as the one produced by ISOMAP. Here we observe that the data is indeed a sheet, but in preserving some of these global potential distances in 2 dimensions, we see that the roll is not completely flattened.\n",
    "\n",
    "**Three blobs** - PHATE on the three blobs perserves the cluster-like nature of the data and projects each cluster onto a single axis of variation within each cluster. The green blob's axis would likely be seen in visualizing in 3 dimensions. Unfortunately, the relative positions of the blue:orange and blue:green blob is close and this visualization makes it seem that the blue and orange blob are equidistant to the green. This is likely due to the graph being sparsely connected between the blue and orange blob but largely disconnected from the green blob.\n",
    "\n",
    "**Uneven circle** - PHATE acturately embeds the circle in two dimensions and preserves the uneven spacing of the points aroung the circle.\n",
    "\n",
    "**Tree** - PHATE accurately captures the structure of the tree in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = pickle.load(open(\"data/parameter_search.pickle\", 'rb'))\n",
    "dataset = data.swissroll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a visualization tool, the user is inevitably faced with the daunting choice of various parameters. Some parameters are robust and barely affect the result of the visualization; others drastically affect the results and need to be tuned with care.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ccf679346547ed84f6bbb9438a18a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': 'PCA', '1': 'MDS', '2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_widget = interact.TabWidget()\n",
    "for algorithm in results.keys():\n",
    "    with tab_widget.new_tab(algorithm):\n",
    "        interact.parameter_plot(algorithm, results, dataset.X_true, c=dataset.c)\n",
    "\n",
    "tab_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The role of real data in comparing visualization algorithms\n",
    "\n",
    "Toy data is useful because it provides insights into the kinds of idiosyncracies that a given algorithm is sensitive to. However, it is also important that a dimensionality reduction algorithm performs well on real world data. Although you lose access to ground truth, it is still possible to compare algorithms. \n",
    "\n",
    "Generally spearking it useful to first consider well existing knowledge is preserved by an given embedding. Are groups of data points that are known to be closely related displayed proximally in the visualization? Can longer-range relationships be observed across the plot?\n",
    "\n",
    "Once these properties have been confirmed, we usually will generate some hypothesis about the data and further examine trends within the data. This might mean gathering additional information about the data by integrating another outside dataset or performing another experiment. Although this process is challenging, the real utility of a visualization is how successful one can be by making hypotheses from the visualization and testing them using an independent method.\n",
    "\n",
    "Here, we will describe three real world datasets and compare each visualization algorithm on each dataset. Again, we're going to use default parameters for every method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of population genetics data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 1000 Genomes Project\n",
    "\n",
    "The [1000 Genomes Project](http://www.internationalgenome.org/) is an international effort to understand genetic variation across human populations. Originating in 2007, the project completed in 2013 describing variation in roughly 3000 individuals from 26 populations across more than 80,000 genetics loci. The project is a high quality resource for understanding differences between people from different regions across the globe.\n",
    "\n",
    "This data is useful for visualization because we have an intuitive understanding of geographic relationships between people, but the data is so high dimensional that understanding if these structures are preserved in the genetic space is infeasible.\n",
    "\n",
    "In the 1000 Genomes data we also have access to detailed population information about each individual in the study. We expect that geographically proximal populations are genotypically similar and should therefore be grouped together in the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the data look like?\n",
    "\n",
    "As mentioned above, the 1000 Genomes dataset contains genetic information about 3000 individuals. The data we will embed is called a genotype matrix. The rows of this matrix correspond to each individual in the study, and each column corresponds to a [Single Nucleotide Polymorphism (SNP)](https://ghr.nlm.nih.gov/primer/genomicresearch/snp). Each entry in the matrix is `0` if the individual is homozygous for the reference allele, `1` if the individual is heterozygous for the reference and alternative alleles, and `2` if the individual is homozygous for the alternative alleles. If these terms are unfamiliar to you, we suggest reading the NIH primer on SNPs. All that's really important to know if that each entry in the matrix indicates a specific DNA element of each individual in the dataset.\n",
    "\n",
    "Thankfully Alex Diaz-Papkovich from Simon Gravel's lab at McGill provides [scripts to load and preprocess this data on Github](https://github.com/diazale/1KGP_dimred). All we need to do is run his scripts through the algorithms for our comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing visualization algorithms on the 1000 Genomes data.\n",
    "\n",
    "![1000 - Genomes comparison](./img/1000_genomes.comparison.2x3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is complex with many different populations spanning six continents. Generally speaking, Blue corresponds to East Asian, Green is South Asain, Yellow/Orange is African, Red represents European, and Pink is Ad Mixed American. The exact breakdown of each population code is given by the following table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Population Code  | Population Description                                             | Super Population  | \n",
    "|------------------|--------------------------------------------------------------------|------------------------| \n",
    "| CHB              | Han Chinese in Beijing, China                                      | East Asian             | \n",
    "| JPT              | Japanese in Tokyo, Japan                                           | East Asian             | \n",
    "| CHS              | Southern Han Chinese                                               | East Asian             | \n",
    "| CDX              | Chinese Dai in Xishuangbanna, China                                | East Asian             | \n",
    "| KHV              | Kinh in Ho Chi Minh City, Vietnam                                  | East Asian             | \n",
    "| CEU              | Utah Residents (CEPH) with Northern and Western European Ancestry  | European               | \n",
    "| TSI              | Toscani in Italia                                                  | European               | \n",
    "| FIN              | Finnish in Finland                                                 | European               | \n",
    "| GBR              | British in England and Scotland                                    | European               | \n",
    "| IBS              | Iberian Population in Spain                                        | European               | \n",
    "| YRI              | Yoruba in Ibadan, Nigeria                                          | African                | \n",
    "| LWK              | Luhya in Webuye, Kenya                                             | African                | \n",
    "| GWD              | Gambian in Western Divisions in the Gambia                         | African                | \n",
    "| MSL              | Mende in Sierra Leone                                              | African                | \n",
    "| ESN              | Esan in Nigeria                                                    | African                | \n",
    "| ASW              | Americans of African Ancestry in SW USA                            | African                | \n",
    "| ACB              | African Caribbeans in Barbados                                     | African                | \n",
    "| MXL              | Mexican Ancestry from Los Angeles USA                              | Ad Mixed American      | \n",
    "| PUR              | Puerto Ricans from Puerto Rico                                     | Ad Mixed American      | \n",
    "| CLM              | Colombians from Medellin, Colombia                                 | Ad Mixed American      | \n",
    "| PEL              | Peruvians from Lima, Peru                                          | Ad Mixed American      | \n",
    "| GIH              | Gujarati Indian from Houston, Texas                                | South Asian            | \n",
    "| PJL              | Punjabi from Lahore, Pakistan                                      | South Asian            | \n",
    "| BEB              | Bengali from Bangladesh                                            | South Asian            | \n",
    "| STU              | Sri Lankan Tamil from the UK                                       | South Asian            | \n",
    "| ITU              | Indian Telugu from the UK                                          | South Asian            | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA** is the most common method for visualizaing population genetics information, but limitations in this method mean that as larger datasets become available, researchers are often searching for other tools [Diaz-Papkovich et al. (2019)](https://www.biorxiv.org/content/10.1101/423632v2). Examining the output of PCA on this dataset, we can see clear separation between the African populations denoted by the Yellow/Orange points from the rest of the populations. Similarly, the East Asian population is separated from the rest. However, it is difficult to make statements about the American, South Asian, or European populations.\n",
    "\n",
    "**MDS** has some artistic appeal here as the visualization resembles a globe. However, we have a problem here of the pink Ad Mixed American population divided in two by the South Asian population. We see this as well with PCA, but this does not reconcile with our understanding of genetic variation between these populations. Generally speaking, following humanity's migration out of Africa has been characterized by increasing genetic isolation with limited examples of population mixing ([Hellenthal et al. 2014.](https://www.ncbi.nlm.nih.gov/pubmed/24531965/); [Norris et al. 2018.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6288849/)).\n",
    "\n",
    "**ISOMAP** performs poorly here. See how the South Asian and East Asian populations are completely reduced to a very small region plot. This poorly represents the diversity of those populations.\n",
    "\n",
    "**TSNE** can be very sensitive to outliers. Here, many individuals have essentially no neighbors in the high dimensional space and therefore they get evenly distributed around the visualization. Not so useful.\n",
    "\n",
    "**UMAP** was shown to work well for population genetics data by [Diaz-Papkovich et al. (2019)](https://www.biorxiv.org/content/10.1101/423632v2), and this was actually the inspriration for this real-world data application. There, the authors used some parameter tuning to generate the plots in the manuscript, so their visualization is slightly nicer than what we see here. However, the game of this blog post is to use default parameters in all comparisons. We see here that the defaults for UMAP produce a visualization that is very compressed, making it hard to see fine-grain relationships between points.\n",
    "\n",
    "**PHATE** does a good job here of presenting each population as a separate group of points on the plots. The default parameters also compress a lot of the variation into a few smooth trajectories. One of the places where PHATE shines here is when you consider the way that orderings of each principle component are preserved within each group of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining PCS along each visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1000 - Genomes comparison](./img/1000_genomes.comparison.PC1.png)\n",
    "\n",
    "![1000 - Genomes comparison](./img/1000_genomes.comparison.PC2.png)\n",
    "\n",
    "![1000 - Genomes comparison](./img/1000_genomes.comparison.PC3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots, we're looking at the first three principal components loadings on each individual in the dataset. If you look at PC1 in PHATE vs UMAP, PHATE preserved the ordering of cells by increasing PC1 loadings whereas UMAP splits the data points with high PC1 coordinates across two different clusters. Examining PC2 and PC3, we see that the embedding produced by MDS and TSNE order points non-monotonically by their coordinate loadings. This is a little frustrating because we generally think of these coordinates as representing axes of diversity throughout each population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of single cell RNA-sequencing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retinal biopolar scRNA-seq data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
